{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bitdribble/LDL/blob/main/colab/c6e1_boston.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaFF7Vl6Lar2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The MIT License (MIT)\n",
        "Copyright (c) 2021 NVIDIA\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
        "this software and associated documentation files (the \"Software\"), to deal in\n",
        "the Software without restriction, including without limitation the rights to\n",
        "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
        "the Software, and to permit persons to whom the Software is furnished to do so,\n",
        "subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
        "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
        "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
        "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
        "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up sandbox environment\n",
        "!rm -rf LDL\n",
        "!git clone https://github.com/Bitdribble/LDL.git\n",
        "\n",
        "# Install module dependencies\n",
        "%cd /content/LDL\n",
        "%pip install -r colab_requirements.txt\n",
        "\n",
        "# Download data\n",
        "#!ls data/mnist\n",
        "#!./data/mnist/download_mnist.sh\n",
        "\n",
        "# cd to desired directory\n",
        "%cd /content/LDL/pt_framework"
      ],
      "metadata": {
        "id": "zgoe8jTvLhtT",
        "outputId": "ace5ddf7-1659-43ab-ec77-428c0a1631aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LDL'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 209 (delta 111), reused 169 (delta 77), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (209/209), 1.21 MiB | 10.60 MiB/s, done.\n",
            "Resolving deltas: 100% (111/111), done.\n",
            "/content/LDL\n",
            "Collecting idx2numpy\n",
            "  Downloading idx2numpy-1.2.3.tar.gz (6.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from idx2numpy->-r colab_requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from idx2numpy->-r colab_requirements.txt (line 3)) (1.15.0)\n",
            "Building wheels for collected packages: idx2numpy\n",
            "  Building wheel for idx2numpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idx2numpy: filename=idx2numpy-1.2.3-py3-none-any.whl size=7919 sha256=dacc68cf5fbd5137bb57b34038674b5821dec7bac4c3236b6570cbaa2974acec\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/ce/ad/d5e95a35cfe34149aade5e500f2edd535c0566d79e9a8e1d8a\n",
            "Successfully built idx2numpy\n",
            "Installing collected packages: idx2numpy\n",
            "Successfully installed idx2numpy-1.2.3\n",
            "/content/LDL/pt_framework\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKYw6pX5Lar8"
      },
      "source": [
        "This code example demonstrates how to use a neural network to solve a regression problem, using the Boston housing dataset. More context for this code example can be found in the section \"Programming Example: Predicting House Prices with a DNN\" in Chapter 6 in the book Learning Deep Learning by Magnus Ekman (ISBN: 9780137470358).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0LTLgVDLar-"
      },
      "source": [
        "Unlike MNIST, the Boston Housing dataset is not included with PyTorch, so we retrieve it using scikit-learn instead. This is done by calling the load_boston() function. We then retrieve the inputs and targets as NumPy arrays by calling the get() method. We explicitly split them up into a training set and a test set using the scikit-learn function train_test_split().\n",
        "\n",
        "We convert the NumPy arrays to np.float32 and reshape them to ensure that the datatype and dimensions later match what PyTorch expects. \n",
        "\n",
        "We standardize both the training and test data by using the mean and standard deviation from the training data. The parameter axis=0 ensures that we compute the mean and standard deviation for each input variable separately. The resulting mean (and standard deviation) is a vector of means instead of a single value. That is, the standardized value of the nitric oxides concentration is not affected by the values of the per capita crime rate or any of the other variables.\n",
        "\n",
        "Finally we create Dataset objects. To do that we need to first convert the NumPy arrays to PyTorch tensors. That is done by calling torch.from_numpy().\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FsBbZPBKLar_",
        "outputId": "c544d724-6977-49f4-a977-f1828f91842c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from utilities import train_model\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "EPOCHS = 500\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Read and standardize the data.\n",
        "boston_housing = load_boston()\n",
        "data = boston_housing.get('data')\n",
        "target = boston_housing.get('target')\n",
        "\n",
        "raw_x_train, raw_x_test, y_train, y_test = train_test_split(\n",
        "    data, target, test_size=0.2, random_state=0)\n",
        "\n",
        "# Convert to same precision as model.\n",
        "raw_x_train = raw_x_train.astype(np.float32)\n",
        "raw_x_test = raw_x_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "y_train = np.reshape(y_train, (-1, 1))\n",
        "y_test = np.reshape(y_test, (-1, 1))\n",
        "\n",
        "x_mean = np.mean(raw_x_train, axis=0)\n",
        "x_stddev = np.std(raw_x_train, axis=0)\n",
        "x_train = (raw_x_train - x_mean) / x_stddev\n",
        "x_test = (raw_x_test - x_mean) / x_stddev\n",
        "\n",
        "# Create Dataset objects.\n",
        "trainset = TensorDataset(torch.from_numpy(x_train),\n",
        "                         torch.from_numpy(y_train))\n",
        "testset = TensorDataset(torch.from_numpy(x_test),\n",
        "                        torch.from_numpy(y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIKi6wHGLasC"
      },
      "source": [
        "We then create the model. The code looks follows the same pattern as c5e1_mnist_learning. We define our network to have two hidden layers, so we are now officially doing DL! The two hidden layers in our network implementation have 64 ReLU neurons each, where the first layer is declared to have 13 inputs to match the dataset. The output layer consists of a single neuron with a linear activation function. We use MSE as the loss function and use the Adam optimizer.\n",
        "\n",
        "Instead of implementing the training loop below, we have broken it out into a separate function train_model(). Its implementation can be found in the file utilities.py. It is very similar to the training loop in c5e1_mnist_learning but has some additional logic to be able to handle both classification and regression problems. In particular, it takes a parameter \"metric\". If we work on a classification problem it should be set to \"acc\" and the function will compute accuracy. If we work on a regression problem it should be set to \"mae\" and the function will compute mean absolute error instead. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zpQdvE_uLasD",
        "outputId": "f3199412-8226-4aa9-ac67-6a00c1f3cb6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500 loss: 545.1769 - mae: 20.7512 - val_loss: 454.2590 - val_mae: 17.7616\n",
            "Epoch 2/500 loss: 408.9080 - mae: 17.3976 - val_loss: 286.4925 - val_mae: 13.4286\n",
            "Epoch 3/500 loss: 200.3347 - mae: 11.6793 - val_loss: 110.9084 - val_mae: 7.7008\n",
            "Epoch 4/500 loss: 62.4472 - mae: 6.1432 - val_loss: 66.5486 - val_mae: 5.5533\n",
            "Epoch 5/500 loss: 37.6736 - mae: 4.4545 - val_loss: 50.4528 - val_mae: 4.9521\n",
            "Epoch 6/500 loss: 27.3155 - mae: 3.7503 - val_loss: 44.2256 - val_mae: 4.6478\n",
            "Epoch 7/500 loss: 25.4908 - mae: 3.5196 - val_loss: 40.1497 - val_mae: 4.4042\n",
            "Epoch 8/500 loss: 21.4497 - mae: 3.3100 - val_loss: 38.3948 - val_mae: 4.2460\n",
            "Epoch 9/500 loss: 19.5296 - mae: 3.1965 - val_loss: 35.3808 - val_mae: 4.0514\n",
            "Epoch 10/500 loss: 18.5765 - mae: 3.0794 - val_loss: 32.4696 - val_mae: 3.8915\n",
            "Epoch 11/500 loss: 16.6848 - mae: 2.9574 - val_loss: 31.2790 - val_mae: 3.7680\n",
            "Epoch 12/500 loss: 15.9561 - mae: 2.8176 - val_loss: 29.0453 - val_mae: 3.6298\n",
            "Epoch 13/500 loss: 14.8895 - mae: 2.7690 - val_loss: 28.3439 - val_mae: 3.5374\n",
            "Epoch 14/500 loss: 13.5955 - mae: 2.6220 - val_loss: 27.6375 - val_mae: 3.4718\n",
            "Epoch 15/500 loss: 12.7820 - mae: 2.5378 - val_loss: 26.0868 - val_mae: 3.3531\n",
            "Epoch 16/500 loss: 11.9084 - mae: 2.4754 - val_loss: 25.6182 - val_mae: 3.3096\n",
            "Epoch 17/500 loss: 11.3013 - mae: 2.3959 - val_loss: 24.1801 - val_mae: 3.1842\n",
            "Epoch 18/500 loss: 10.9505 - mae: 2.3601 - val_loss: 24.6361 - val_mae: 3.1953\n",
            "Epoch 19/500 loss: 10.5071 - mae: 2.3302 - val_loss: 22.8006 - val_mae: 3.0713\n",
            "Epoch 20/500 loss: 10.1985 - mae: 2.2615 - val_loss: 23.9899 - val_mae: 3.1237\n",
            "Epoch 21/500 loss: 9.7178 - mae: 2.2225 - val_loss: 23.0000 - val_mae: 3.0395\n",
            "Epoch 22/500 loss: 9.5956 - mae: 2.2038 - val_loss: 22.4017 - val_mae: 3.0194\n",
            "Epoch 23/500 loss: 9.2568 - mae: 2.1606 - val_loss: 22.3999 - val_mae: 2.9772\n",
            "Epoch 24/500 loss: 8.8833 - mae: 2.1379 - val_loss: 21.3544 - val_mae: 2.9534\n",
            "Epoch 25/500 loss: 9.0563 - mae: 2.1638 - val_loss: 21.9263 - val_mae: 2.9798\n",
            "Epoch 26/500 loss: 8.7443 - mae: 2.1425 - val_loss: 21.8748 - val_mae: 2.9500\n",
            "Epoch 27/500 loss: 8.4711 - mae: 2.0880 - val_loss: 20.9521 - val_mae: 2.9281\n",
            "Epoch 28/500 loss: 10.1175 - mae: 2.0475 - val_loss: 21.0888 - val_mae: 2.9035\n",
            "Epoch 29/500 loss: 8.8172 - mae: 2.1437 - val_loss: 21.6775 - val_mae: 2.9327\n",
            "Epoch 30/500 loss: 8.3026 - mae: 2.0987 - val_loss: 21.3998 - val_mae: 2.9536\n",
            "Epoch 31/500 loss: 7.8627 - mae: 2.0170 - val_loss: 20.6378 - val_mae: 2.8856\n",
            "Epoch 32/500 loss: 7.6235 - mae: 1.9763 - val_loss: 20.9134 - val_mae: 2.8994\n",
            "Epoch 33/500 loss: 7.6720 - mae: 1.9778 - val_loss: 20.7153 - val_mae: 2.8843\n",
            "Epoch 34/500 loss: 7.8595 - mae: 1.9560 - val_loss: 21.2851 - val_mae: 2.8951\n",
            "Epoch 35/500 loss: 7.4896 - mae: 1.9485 - val_loss: 20.2833 - val_mae: 2.8550\n",
            "Epoch 36/500 loss: 7.5046 - mae: 1.9348 - val_loss: 21.1719 - val_mae: 2.9143\n",
            "Epoch 37/500 loss: 7.6260 - mae: 1.9336 - val_loss: 19.3772 - val_mae: 2.8519\n",
            "Epoch 38/500 loss: 7.2556 - mae: 1.9405 - val_loss: 20.7603 - val_mae: 2.8542\n",
            "Epoch 39/500 loss: 6.8147 - mae: 1.8822 - val_loss: 19.7033 - val_mae: 2.8170\n",
            "Epoch 40/500 loss: 7.4159 - mae: 1.8947 - val_loss: 20.2247 - val_mae: 2.8648\n",
            "Epoch 41/500 loss: 6.9231 - mae: 1.8907 - val_loss: 19.7897 - val_mae: 2.8364\n",
            "Epoch 42/500 loss: 6.7140 - mae: 1.8741 - val_loss: 19.9992 - val_mae: 2.8389\n",
            "Epoch 43/500 loss: 6.5525 - mae: 1.8463 - val_loss: 19.8964 - val_mae: 2.8637\n",
            "Epoch 44/500 loss: 6.7240 - mae: 1.8432 - val_loss: 19.6605 - val_mae: 2.8315\n",
            "Epoch 45/500 loss: 6.5747 - mae: 1.8227 - val_loss: 20.3205 - val_mae: 2.8547\n",
            "Epoch 46/500 loss: 6.3999 - mae: 1.8525 - val_loss: 19.4967 - val_mae: 2.8268\n",
            "Epoch 47/500 loss: 6.3529 - mae: 1.8213 - val_loss: 19.3221 - val_mae: 2.8310\n",
            "Epoch 48/500 loss: 6.3179 - mae: 1.8062 - val_loss: 18.8278 - val_mae: 2.7948\n",
            "Epoch 49/500 loss: 6.3032 - mae: 1.7954 - val_loss: 19.6166 - val_mae: 2.8261\n",
            "Epoch 50/500 loss: 6.1007 - mae: 1.7883 - val_loss: 20.4632 - val_mae: 2.9340\n",
            "Epoch 51/500 loss: 6.3382 - mae: 1.7934 - val_loss: 18.6441 - val_mae: 2.8124\n",
            "Epoch 52/500 loss: 6.0464 - mae: 1.7589 - val_loss: 19.0238 - val_mae: 2.7891\n",
            "Epoch 53/500 loss: 5.8658 - mae: 1.7402 - val_loss: 18.9916 - val_mae: 2.8125\n",
            "Epoch 54/500 loss: 5.9474 - mae: 1.7454 - val_loss: 18.5584 - val_mae: 2.8049\n",
            "Epoch 55/500 loss: 5.7533 - mae: 1.7238 - val_loss: 19.5127 - val_mae: 2.8080\n",
            "Epoch 56/500 loss: 5.8614 - mae: 1.7202 - val_loss: 18.7699 - val_mae: 2.8007\n",
            "Epoch 57/500 loss: 6.8654 - mae: 1.7617 - val_loss: 18.8507 - val_mae: 2.7918\n",
            "Epoch 58/500 loss: 5.7320 - mae: 1.7719 - val_loss: 19.4450 - val_mae: 2.8565\n",
            "Epoch 59/500 loss: 5.6981 - mae: 1.7457 - val_loss: 18.8357 - val_mae: 2.7958\n",
            "Epoch 60/500 loss: 5.4715 - mae: 1.7031 - val_loss: 18.8347 - val_mae: 2.8089\n",
            "Epoch 61/500 loss: 5.2984 - mae: 1.6967 - val_loss: 18.9144 - val_mae: 2.7969\n",
            "Epoch 62/500 loss: 5.2153 - mae: 1.6657 - val_loss: 18.1331 - val_mae: 2.7702\n",
            "Epoch 63/500 loss: 5.2478 - mae: 1.6789 - val_loss: 18.6489 - val_mae: 2.7878\n",
            "Epoch 64/500 loss: 5.2805 - mae: 1.6347 - val_loss: 18.8378 - val_mae: 2.8237\n",
            "Epoch 65/500 loss: 5.2692 - mae: 1.7023 - val_loss: 18.5098 - val_mae: 2.7706\n",
            "Epoch 66/500 loss: 5.0934 - mae: 1.6448 - val_loss: 18.0268 - val_mae: 2.7804\n",
            "Epoch 67/500 loss: 5.0587 - mae: 1.6504 - val_loss: 18.6841 - val_mae: 2.7757\n",
            "Epoch 68/500 loss: 5.1849 - mae: 1.6842 - val_loss: 19.5330 - val_mae: 2.8166\n",
            "Epoch 69/500 loss: 4.8405 - mae: 1.6306 - val_loss: 17.8198 - val_mae: 2.7726\n",
            "Epoch 70/500 loss: 5.4612 - mae: 1.6111 - val_loss: 17.8123 - val_mae: 2.7630\n",
            "Epoch 71/500 loss: 5.0022 - mae: 1.6429 - val_loss: 17.8242 - val_mae: 2.7638\n",
            "Epoch 72/500 loss: 5.1821 - mae: 1.6485 - val_loss: 17.5975 - val_mae: 2.7639\n",
            "Epoch 73/500 loss: 4.8475 - mae: 1.5980 - val_loss: 17.4944 - val_mae: 2.7626\n",
            "Epoch 74/500 loss: 4.9769 - mae: 1.6492 - val_loss: 18.0870 - val_mae: 2.7545\n",
            "Epoch 75/500 loss: 4.7153 - mae: 1.5761 - val_loss: 17.9329 - val_mae: 2.7529\n",
            "Epoch 76/500 loss: 4.7742 - mae: 1.5716 - val_loss: 18.5537 - val_mae: 2.7902\n",
            "Epoch 77/500 loss: 4.5466 - mae: 1.5692 - val_loss: 17.6874 - val_mae: 2.7557\n",
            "Epoch 78/500 loss: 4.7790 - mae: 1.5766 - val_loss: 18.1655 - val_mae: 2.7470\n",
            "Epoch 79/500 loss: 4.7539 - mae: 1.5663 - val_loss: 17.7465 - val_mae: 2.7395\n",
            "Epoch 80/500 loss: 4.5741 - mae: 1.5663 - val_loss: 17.7832 - val_mae: 2.7772\n",
            "Epoch 81/500 loss: 4.7687 - mae: 1.5999 - val_loss: 17.9840 - val_mae: 2.7566\n",
            "Epoch 82/500 loss: 4.5572 - mae: 1.5691 - val_loss: 19.1793 - val_mae: 2.7891\n",
            "Epoch 83/500 loss: 4.7737 - mae: 1.5933 - val_loss: 17.3461 - val_mae: 2.7417\n",
            "Epoch 84/500 loss: 4.5355 - mae: 1.5210 - val_loss: 18.1986 - val_mae: 2.7427\n",
            "Epoch 85/500 loss: 4.3905 - mae: 1.5192 - val_loss: 17.6294 - val_mae: 2.7188\n",
            "Epoch 86/500 loss: 4.3380 - mae: 1.5270 - val_loss: 17.6085 - val_mae: 2.7355\n",
            "Epoch 87/500 loss: 4.5785 - mae: 1.5069 - val_loss: 18.4852 - val_mae: 2.7469\n",
            "Epoch 88/500 loss: 4.1966 - mae: 1.5284 - val_loss: 17.4359 - val_mae: 2.7411\n",
            "Epoch 89/500 loss: 4.2204 - mae: 1.5064 - val_loss: 17.8302 - val_mae: 2.7414\n",
            "Epoch 90/500 loss: 4.1330 - mae: 1.4834 - val_loss: 17.8688 - val_mae: 2.7400\n",
            "Epoch 91/500 loss: 3.9926 - mae: 1.4774 - val_loss: 17.5102 - val_mae: 2.7182\n",
            "Epoch 92/500 loss: 4.0274 - mae: 1.4778 - val_loss: 17.8138 - val_mae: 2.7418\n",
            "Epoch 93/500 loss: 3.9714 - mae: 1.4740 - val_loss: 17.9337 - val_mae: 2.7447\n",
            "Epoch 94/500 loss: 4.3277 - mae: 1.4856 - val_loss: 17.4207 - val_mae: 2.8057\n",
            "Epoch 95/500 loss: 4.0825 - mae: 1.4942 - val_loss: 17.6217 - val_mae: 2.7347\n",
            "Epoch 96/500 loss: 3.9507 - mae: 1.4565 - val_loss: 18.4640 - val_mae: 2.7444\n",
            "Epoch 97/500 loss: 3.9904 - mae: 1.4400 - val_loss: 17.2999 - val_mae: 2.7148\n",
            "Epoch 98/500 loss: 3.9490 - mae: 1.4457 - val_loss: 17.6839 - val_mae: 2.7355\n",
            "Epoch 99/500 loss: 3.8211 - mae: 1.4319 - val_loss: 17.6681 - val_mae: 2.7311\n",
            "Epoch 100/500 loss: 3.7840 - mae: 1.4316 - val_loss: 17.8806 - val_mae: 2.7413\n",
            "Epoch 101/500 loss: 3.6740 - mae: 1.4139 - val_loss: 17.5204 - val_mae: 2.7170\n",
            "Epoch 102/500 loss: 3.6805 - mae: 1.4259 - val_loss: 17.1465 - val_mae: 2.7098\n",
            "Epoch 103/500 loss: 3.8498 - mae: 1.4054 - val_loss: 17.0915 - val_mae: 2.7264\n",
            "Epoch 104/500 loss: 3.8841 - mae: 1.4378 - val_loss: 18.2709 - val_mae: 2.7403\n",
            "Epoch 105/500 loss: 3.8334 - mae: 1.4099 - val_loss: 18.7452 - val_mae: 2.7641\n",
            "Epoch 106/500 loss: 4.0560 - mae: 1.4698 - val_loss: 17.2389 - val_mae: 2.6914\n",
            "Epoch 107/500 loss: 3.7343 - mae: 1.4079 - val_loss: 17.8396 - val_mae: 2.7095\n",
            "Epoch 108/500 loss: 3.6486 - mae: 1.3832 - val_loss: 17.1452 - val_mae: 2.6881\n",
            "Epoch 109/500 loss: 3.4381 - mae: 1.3671 - val_loss: 17.4190 - val_mae: 2.7129\n",
            "Epoch 110/500 loss: 3.5125 - mae: 1.3730 - val_loss: 17.1681 - val_mae: 2.7233\n",
            "Epoch 111/500 loss: 3.6066 - mae: 1.3388 - val_loss: 17.7625 - val_mae: 2.7016\n",
            "Epoch 112/500 loss: 3.4021 - mae: 1.3608 - val_loss: 17.5014 - val_mae: 2.7069\n",
            "Epoch 113/500 loss: 3.3522 - mae: 1.3502 - val_loss: 16.7897 - val_mae: 2.7087\n",
            "Epoch 114/500 loss: 3.5212 - mae: 1.3664 - val_loss: 17.4297 - val_mae: 2.7716\n",
            "Epoch 115/500 loss: 3.4715 - mae: 1.3750 - val_loss: 16.9753 - val_mae: 2.7044\n",
            "Epoch 116/500 loss: 3.2530 - mae: 1.3309 - val_loss: 17.7673 - val_mae: 2.7162\n",
            "Epoch 117/500 loss: 3.2639 - mae: 1.3274 - val_loss: 16.8699 - val_mae: 2.7169\n",
            "Epoch 118/500 loss: 3.4327 - mae: 1.3237 - val_loss: 17.6626 - val_mae: 2.7021\n",
            "Epoch 119/500 loss: 3.2140 - mae: 1.3128 - val_loss: 16.9823 - val_mae: 2.6886\n",
            "Epoch 120/500 loss: 3.2852 - mae: 1.3294 - val_loss: 17.5261 - val_mae: 2.7298\n",
            "Epoch 121/500 loss: 3.5885 - mae: 1.3140 - val_loss: 16.8000 - val_mae: 2.7005\n",
            "Epoch 122/500 loss: 3.2610 - mae: 1.3380 - val_loss: 17.3778 - val_mae: 2.7077\n",
            "Epoch 123/500 loss: 3.1183 - mae: 1.2805 - val_loss: 17.4265 - val_mae: 2.7013\n",
            "Epoch 124/500 loss: 3.1549 - mae: 1.2859 - val_loss: 17.7610 - val_mae: 2.7282\n",
            "Epoch 125/500 loss: 3.0969 - mae: 1.2897 - val_loss: 17.2582 - val_mae: 2.6917\n",
            "Epoch 126/500 loss: 3.1126 - mae: 1.2764 - val_loss: 17.3892 - val_mae: 2.6813\n",
            "Epoch 127/500 loss: 3.1352 - mae: 1.2669 - val_loss: 17.2773 - val_mae: 2.6818\n",
            "Epoch 128/500 loss: 3.1765 - mae: 1.3129 - val_loss: 17.7306 - val_mae: 2.6953\n",
            "Epoch 129/500 loss: 2.9901 - mae: 1.2581 - val_loss: 17.1323 - val_mae: 2.6690\n",
            "Epoch 130/500 loss: 3.2384 - mae: 1.3168 - val_loss: 16.8798 - val_mae: 2.6817\n",
            "Epoch 131/500 loss: 3.1663 - mae: 1.2798 - val_loss: 17.3372 - val_mae: 2.7289\n",
            "Epoch 132/500 loss: 3.0645 - mae: 1.2777 - val_loss: 17.8922 - val_mae: 2.7567\n",
            "Epoch 133/500 loss: 3.0434 - mae: 1.2633 - val_loss: 17.6211 - val_mae: 2.6976\n",
            "Epoch 134/500 loss: 2.8556 - mae: 1.2403 - val_loss: 17.2382 - val_mae: 2.6982\n",
            "Epoch 135/500 loss: 2.8691 - mae: 1.2251 - val_loss: 17.1387 - val_mae: 2.7069\n",
            "Epoch 136/500 loss: 2.8264 - mae: 1.2077 - val_loss: 17.8749 - val_mae: 2.7098\n",
            "Epoch 137/500 loss: 2.8420 - mae: 1.2220 - val_loss: 18.0121 - val_mae: 2.7474\n",
            "Epoch 138/500 loss: 3.0528 - mae: 1.2041 - val_loss: 17.4733 - val_mae: 2.7191\n",
            "Epoch 139/500 loss: 2.9019 - mae: 1.2408 - val_loss: 17.6359 - val_mae: 2.7222\n",
            "Epoch 140/500 loss: 2.8169 - mae: 1.2092 - val_loss: 17.5228 - val_mae: 2.7186\n",
            "Epoch 141/500 loss: 2.9890 - mae: 1.2110 - val_loss: 17.2436 - val_mae: 2.7168\n",
            "Epoch 142/500 loss: 2.8856 - mae: 1.2325 - val_loss: 17.5973 - val_mae: 2.7697\n",
            "Epoch 143/500 loss: 2.8125 - mae: 1.2189 - val_loss: 18.0292 - val_mae: 2.7238\n",
            "Epoch 144/500 loss: 2.7948 - mae: 1.2082 - val_loss: 17.7005 - val_mae: 2.7096\n",
            "Epoch 145/500 loss: 2.7958 - mae: 1.2079 - val_loss: 17.6163 - val_mae: 2.8119\n",
            "Epoch 146/500 loss: 2.8276 - mae: 1.2100 - val_loss: 17.6600 - val_mae: 2.6950\n",
            "Epoch 147/500 loss: 2.7299 - mae: 1.1966 - val_loss: 17.4179 - val_mae: 2.7113\n",
            "Epoch 148/500 loss: 2.6814 - mae: 1.1839 - val_loss: 16.8906 - val_mae: 2.6855\n",
            "Epoch 149/500 loss: 2.6605 - mae: 1.1797 - val_loss: 18.0748 - val_mae: 2.7582\n",
            "Epoch 150/500 loss: 2.8440 - mae: 1.2479 - val_loss: 18.7330 - val_mae: 2.7438\n",
            "Epoch 151/500 loss: 2.7275 - mae: 1.2078 - val_loss: 17.5586 - val_mae: 2.7103\n",
            "Epoch 152/500 loss: 2.4906 - mae: 1.1616 - val_loss: 18.1690 - val_mae: 2.7547\n",
            "Epoch 153/500 loss: 2.6518 - mae: 1.1540 - val_loss: 17.4286 - val_mae: 2.7488\n",
            "Epoch 154/500 loss: 2.7162 - mae: 1.2153 - val_loss: 17.9166 - val_mae: 2.7051\n",
            "Epoch 155/500 loss: 2.5621 - mae: 1.1660 - val_loss: 17.6802 - val_mae: 2.7031\n",
            "Epoch 156/500 loss: 2.5068 - mae: 1.1478 - val_loss: 16.7423 - val_mae: 2.6832\n",
            "Epoch 157/500 loss: 2.6285 - mae: 1.1912 - val_loss: 17.2527 - val_mae: 2.6868\n",
            "Epoch 158/500 loss: 2.6965 - mae: 1.1902 - val_loss: 16.9815 - val_mae: 2.7019\n",
            "Epoch 159/500 loss: 2.5340 - mae: 1.1117 - val_loss: 16.7738 - val_mae: 2.6922\n",
            "Epoch 160/500 loss: 2.5094 - mae: 1.1401 - val_loss: 17.5433 - val_mae: 2.7094\n",
            "Epoch 161/500 loss: 2.4396 - mae: 1.1206 - val_loss: 16.7893 - val_mae: 2.7360\n",
            "Epoch 162/500 loss: 2.5199 - mae: 1.1459 - val_loss: 16.6009 - val_mae: 2.7312\n",
            "Epoch 163/500 loss: 2.4425 - mae: 1.1479 - val_loss: 17.8430 - val_mae: 2.7275\n",
            "Epoch 164/500 loss: 2.4647 - mae: 1.1334 - val_loss: 16.9508 - val_mae: 2.7103\n",
            "Epoch 165/500 loss: 2.4776 - mae: 1.1423 - val_loss: 17.3434 - val_mae: 2.7281\n",
            "Epoch 166/500 loss: 2.3061 - mae: 1.0888 - val_loss: 16.8040 - val_mae: 2.6904\n",
            "Epoch 167/500 loss: 2.5181 - mae: 1.1525 - val_loss: 17.4674 - val_mae: 2.6919\n",
            "Epoch 168/500 loss: 2.6225 - mae: 1.1896 - val_loss: 17.9379 - val_mae: 2.6999\n",
            "Epoch 169/500 loss: 2.7087 - mae: 1.1964 - val_loss: 17.4569 - val_mae: 2.7363\n",
            "Epoch 170/500 loss: 2.6534 - mae: 1.1521 - val_loss: 17.0807 - val_mae: 2.7356\n",
            "Epoch 171/500 loss: 2.3165 - mae: 1.1004 - val_loss: 17.7224 - val_mae: 2.7186\n",
            "Epoch 172/500 loss: 2.4877 - mae: 1.1474 - val_loss: 16.9198 - val_mae: 2.7421\n",
            "Epoch 173/500 loss: 2.8063 - mae: 1.1255 - val_loss: 17.4897 - val_mae: 2.7961\n",
            "Epoch 174/500 loss: 2.4142 - mae: 1.1235 - val_loss: 17.2260 - val_mae: 2.7166\n",
            "Epoch 175/500 loss: 2.2266 - mae: 1.0799 - val_loss: 17.4768 - val_mae: 2.7528\n",
            "Epoch 176/500 loss: 2.2446 - mae: 1.0758 - val_loss: 17.5912 - val_mae: 2.7205\n",
            "Epoch 177/500 loss: 2.2813 - mae: 1.1034 - val_loss: 17.2988 - val_mae: 2.7088\n",
            "Epoch 178/500 loss: 2.3936 - mae: 1.1071 - val_loss: 16.9537 - val_mae: 2.7116\n",
            "Epoch 179/500 loss: 2.2587 - mae: 1.0898 - val_loss: 18.1618 - val_mae: 2.8151\n",
            "Epoch 180/500 loss: 2.3831 - mae: 1.1330 - val_loss: 17.4278 - val_mae: 2.7106\n",
            "Epoch 181/500 loss: 2.2713 - mae: 1.0816 - val_loss: 17.7818 - val_mae: 2.7307\n",
            "Epoch 182/500 loss: 2.2463 - mae: 1.0763 - val_loss: 17.2011 - val_mae: 2.7171\n",
            "Epoch 183/500 loss: 2.2180 - mae: 1.0563 - val_loss: 17.4196 - val_mae: 2.7185\n",
            "Epoch 184/500 loss: 2.3044 - mae: 1.0978 - val_loss: 16.8873 - val_mae: 2.7778\n",
            "Epoch 185/500 loss: 2.2300 - mae: 1.0793 - val_loss: 17.4636 - val_mae: 2.7510\n",
            "Epoch 186/500 loss: 2.1926 - mae: 1.0203 - val_loss: 17.4721 - val_mae: 2.7716\n",
            "Epoch 187/500 loss: 2.4903 - mae: 1.0615 - val_loss: 17.9326 - val_mae: 2.7796\n",
            "Epoch 188/500 loss: 2.1500 - mae: 1.0629 - val_loss: 16.8995 - val_mae: 2.7186\n",
            "Epoch 189/500 loss: 2.1462 - mae: 1.0609 - val_loss: 17.9424 - val_mae: 2.7352\n",
            "Epoch 190/500 loss: 2.1678 - mae: 1.0856 - val_loss: 17.5896 - val_mae: 2.7435\n",
            "Epoch 191/500 loss: 2.2237 - mae: 1.0967 - val_loss: 17.2589 - val_mae: 2.7205\n",
            "Epoch 192/500 loss: 2.0937 - mae: 1.0581 - val_loss: 17.2976 - val_mae: 2.7574\n",
            "Epoch 193/500 loss: 2.2130 - mae: 1.0381 - val_loss: 16.7524 - val_mae: 2.7271\n",
            "Epoch 194/500 loss: 2.2927 - mae: 1.1131 - val_loss: 17.0660 - val_mae: 2.7290\n",
            "Epoch 195/500 loss: 2.0476 - mae: 1.0278 - val_loss: 17.1787 - val_mae: 2.7119\n",
            "Epoch 196/500 loss: 1.9899 - mae: 1.0083 - val_loss: 17.2934 - val_mae: 2.7483\n",
            "Epoch 197/500 loss: 2.1189 - mae: 1.0184 - val_loss: 17.6034 - val_mae: 2.7116\n",
            "Epoch 198/500 loss: 1.9813 - mae: 1.0357 - val_loss: 17.1201 - val_mae: 2.7159\n",
            "Epoch 199/500 loss: 2.1025 - mae: 1.0529 - val_loss: 17.3392 - val_mae: 2.7168\n",
            "Epoch 200/500 loss: 2.0859 - mae: 1.0354 - val_loss: 16.8192 - val_mae: 2.7113\n",
            "Epoch 201/500 loss: 2.1863 - mae: 1.0614 - val_loss: 17.1182 - val_mae: 2.7243\n",
            "Epoch 202/500 loss: 1.9571 - mae: 1.0199 - val_loss: 17.9342 - val_mae: 2.7338\n",
            "Epoch 203/500 loss: 2.1340 - mae: 1.0500 - val_loss: 17.1939 - val_mae: 2.7221\n",
            "Epoch 204/500 loss: 2.1969 - mae: 0.9955 - val_loss: 18.0896 - val_mae: 2.7767\n",
            "Epoch 205/500 loss: 2.2627 - mae: 1.0794 - val_loss: 17.7638 - val_mae: 2.7381\n",
            "Epoch 206/500 loss: 1.9646 - mae: 1.0268 - val_loss: 17.6560 - val_mae: 2.7547\n",
            "Epoch 207/500 loss: 1.9771 - mae: 1.0222 - val_loss: 17.7605 - val_mae: 2.7208\n",
            "Epoch 208/500 loss: 2.0465 - mae: 1.0158 - val_loss: 17.6811 - val_mae: 2.7510\n",
            "Epoch 209/500 loss: 1.9859 - mae: 0.9912 - val_loss: 17.4446 - val_mae: 2.7300\n",
            "Epoch 210/500 loss: 2.5204 - mae: 1.0741 - val_loss: 17.2297 - val_mae: 2.7174\n",
            "Epoch 211/500 loss: 1.9957 - mae: 1.0303 - val_loss: 18.2071 - val_mae: 2.8078\n",
            "Epoch 212/500 loss: 2.0796 - mae: 1.0562 - val_loss: 18.4326 - val_mae: 2.8294\n",
            "Epoch 213/500 loss: 2.0522 - mae: 1.0414 - val_loss: 17.0979 - val_mae: 2.7475\n",
            "Epoch 214/500 loss: 1.9938 - mae: 1.0123 - val_loss: 17.0817 - val_mae: 2.7438\n",
            "Epoch 215/500 loss: 2.0521 - mae: 1.0313 - val_loss: 17.5752 - val_mae: 2.7381\n",
            "Epoch 216/500 loss: 1.9575 - mae: 0.9952 - val_loss: 17.5206 - val_mae: 2.7778\n",
            "Epoch 217/500 loss: 1.8889 - mae: 1.0004 - val_loss: 17.5285 - val_mae: 2.7324\n",
            "Epoch 218/500 loss: 1.8428 - mae: 0.9684 - val_loss: 17.4359 - val_mae: 2.7588\n",
            "Epoch 219/500 loss: 1.8114 - mae: 0.9515 - val_loss: 16.9543 - val_mae: 2.7307\n",
            "Epoch 220/500 loss: 1.8486 - mae: 0.9860 - val_loss: 17.9289 - val_mae: 2.7555\n",
            "Epoch 221/500 loss: 1.9083 - mae: 0.9972 - val_loss: 16.9703 - val_mae: 2.7196\n",
            "Epoch 222/500 loss: 1.8190 - mae: 0.9628 - val_loss: 17.8694 - val_mae: 2.7512\n",
            "Epoch 223/500 loss: 1.8900 - mae: 0.9482 - val_loss: 17.4158 - val_mae: 2.7282\n",
            "Epoch 224/500 loss: 1.9557 - mae: 0.9955 - val_loss: 17.3633 - val_mae: 2.7504\n",
            "Epoch 225/500 loss: 1.7833 - mae: 0.9810 - val_loss: 17.7016 - val_mae: 2.7636\n",
            "Epoch 226/500 loss: 1.8088 - mae: 0.9470 - val_loss: 17.3621 - val_mae: 2.7281\n",
            "Epoch 227/500 loss: 1.7558 - mae: 0.9579 - val_loss: 17.7676 - val_mae: 2.7531\n",
            "Epoch 228/500 loss: 1.6881 - mae: 0.9249 - val_loss: 17.4802 - val_mae: 2.7551\n",
            "Epoch 229/500 loss: 1.8642 - mae: 0.9444 - val_loss: 17.8569 - val_mae: 2.7625\n",
            "Epoch 230/500 loss: 1.7909 - mae: 0.9855 - val_loss: 18.2302 - val_mae: 2.8656\n",
            "Epoch 231/500 loss: 1.8921 - mae: 1.0103 - val_loss: 17.8601 - val_mae: 2.7517\n",
            "Epoch 232/500 loss: 1.8680 - mae: 1.0066 - val_loss: 17.7506 - val_mae: 2.7403\n",
            "Epoch 233/500 loss: 1.7825 - mae: 0.9624 - val_loss: 18.1501 - val_mae: 2.7856\n",
            "Epoch 234/500 loss: 2.0127 - mae: 0.9363 - val_loss: 18.0571 - val_mae: 2.7322\n",
            "Epoch 235/500 loss: 1.8971 - mae: 0.9914 - val_loss: 17.1537 - val_mae: 2.7697\n",
            "Epoch 236/500 loss: 1.8549 - mae: 0.9403 - val_loss: 17.9843 - val_mae: 2.7620\n",
            "Epoch 237/500 loss: 1.7253 - mae: 0.9509 - val_loss: 18.2088 - val_mae: 2.7881\n",
            "Epoch 238/500 loss: 1.7393 - mae: 0.9333 - val_loss: 17.1992 - val_mae: 2.7539\n",
            "Epoch 239/500 loss: 1.7284 - mae: 0.9392 - val_loss: 17.7197 - val_mae: 2.7481\n",
            "Epoch 240/500 loss: 1.6525 - mae: 0.9155 - val_loss: 17.4509 - val_mae: 2.7718\n",
            "Epoch 241/500 loss: 1.8029 - mae: 0.9829 - val_loss: 18.0421 - val_mae: 2.8232\n",
            "Epoch 242/500 loss: 1.7750 - mae: 0.9959 - val_loss: 19.1182 - val_mae: 2.8439\n",
            "Epoch 243/500 loss: 1.9587 - mae: 1.0182 - val_loss: 17.2827 - val_mae: 2.7291\n",
            "Epoch 244/500 loss: 1.7686 - mae: 0.9392 - val_loss: 17.5117 - val_mae: 2.7711\n",
            "Epoch 245/500 loss: 1.6229 - mae: 0.9114 - val_loss: 17.5350 - val_mae: 2.8088\n",
            "Epoch 246/500 loss: 1.8366 - mae: 0.9857 - val_loss: 17.6063 - val_mae: 2.7888\n",
            "Epoch 247/500 loss: 1.7684 - mae: 0.9700 - val_loss: 17.5606 - val_mae: 2.7742\n",
            "Epoch 248/500 loss: 1.6087 - mae: 0.9063 - val_loss: 17.2306 - val_mae: 2.7139\n",
            "Epoch 249/500 loss: 1.6576 - mae: 0.9124 - val_loss: 17.6678 - val_mae: 2.7370\n",
            "Epoch 250/500 loss: 1.6779 - mae: 0.9093 - val_loss: 17.3142 - val_mae: 2.7359\n",
            "Epoch 251/500 loss: 1.6277 - mae: 0.8896 - val_loss: 18.5273 - val_mae: 2.8149\n",
            "Epoch 252/500 loss: 1.7024 - mae: 0.9560 - val_loss: 18.3774 - val_mae: 2.7730\n",
            "Epoch 253/500 loss: 1.6576 - mae: 0.9336 - val_loss: 17.0363 - val_mae: 2.7329\n",
            "Epoch 254/500 loss: 1.7017 - mae: 0.9363 - val_loss: 17.8038 - val_mae: 2.7327\n",
            "Epoch 255/500 loss: 1.7138 - mae: 0.9293 - val_loss: 17.2986 - val_mae: 2.7401\n",
            "Epoch 256/500 loss: 1.6409 - mae: 0.9214 - val_loss: 18.5344 - val_mae: 2.7988\n",
            "Epoch 257/500 loss: 1.5551 - mae: 0.9093 - val_loss: 17.4939 - val_mae: 2.7412\n",
            "Epoch 258/500 loss: 1.6806 - mae: 0.9503 - val_loss: 17.4835 - val_mae: 2.7516\n",
            "Epoch 259/500 loss: 1.8172 - mae: 0.9952 - val_loss: 19.5770 - val_mae: 2.8402\n",
            "Epoch 260/500 loss: 1.8423 - mae: 1.0145 - val_loss: 17.2418 - val_mae: 2.7563\n",
            "Epoch 261/500 loss: 1.6700 - mae: 0.9202 - val_loss: 17.6587 - val_mae: 2.7560\n",
            "Epoch 262/500 loss: 1.9608 - mae: 0.9827 - val_loss: 19.4066 - val_mae: 2.8057\n",
            "Epoch 263/500 loss: 1.6624 - mae: 0.9109 - val_loss: 18.1250 - val_mae: 2.7811\n",
            "Epoch 264/500 loss: 1.5489 - mae: 0.9105 - val_loss: 17.7872 - val_mae: 2.7368\n",
            "Epoch 265/500 loss: 1.5882 - mae: 0.9043 - val_loss: 17.8089 - val_mae: 2.7556\n",
            "Epoch 266/500 loss: 1.5081 - mae: 0.8666 - val_loss: 17.6815 - val_mae: 2.7912\n",
            "Epoch 267/500 loss: 1.5117 - mae: 0.9038 - val_loss: 17.8921 - val_mae: 2.7905\n",
            "Epoch 268/500 loss: 1.4574 - mae: 0.8554 - val_loss: 17.8216 - val_mae: 2.7614\n",
            "Epoch 269/500 loss: 1.5089 - mae: 0.8770 - val_loss: 19.0987 - val_mae: 2.7744\n",
            "Epoch 270/500 loss: 1.7620 - mae: 0.9636 - val_loss: 17.4199 - val_mae: 2.7234\n",
            "Epoch 271/500 loss: 1.4822 - mae: 0.8503 - val_loss: 17.8848 - val_mae: 2.7573\n",
            "Epoch 272/500 loss: 1.4365 - mae: 0.8541 - val_loss: 18.0216 - val_mae: 2.7627\n",
            "Epoch 273/500 loss: 1.4803 - mae: 0.8693 - val_loss: 17.7927 - val_mae: 2.7426\n",
            "Epoch 274/500 loss: 1.3637 - mae: 0.8268 - val_loss: 17.9745 - val_mae: 2.7623\n",
            "Epoch 275/500 loss: 1.3571 - mae: 0.8216 - val_loss: 18.1560 - val_mae: 2.8063\n",
            "Epoch 276/500 loss: 1.5152 - mae: 0.8462 - val_loss: 17.7984 - val_mae: 2.7475\n",
            "Epoch 277/500 loss: 1.5303 - mae: 0.8788 - val_loss: 18.4549 - val_mae: 2.7887\n",
            "Epoch 278/500 loss: 1.5793 - mae: 0.9060 - val_loss: 17.4570 - val_mae: 2.7395\n",
            "Epoch 279/500 loss: 1.4497 - mae: 0.8628 - val_loss: 17.9939 - val_mae: 2.7473\n",
            "Epoch 280/500 loss: 1.4159 - mae: 0.8259 - val_loss: 17.7479 - val_mae: 2.7785\n",
            "Epoch 281/500 loss: 1.3859 - mae: 0.8293 - val_loss: 18.2788 - val_mae: 2.7681\n",
            "Epoch 282/500 loss: 1.3360 - mae: 0.8336 - val_loss: 18.7620 - val_mae: 2.8470\n",
            "Epoch 283/500 loss: 1.4740 - mae: 0.8651 - val_loss: 18.6990 - val_mae: 2.8215\n",
            "Epoch 284/500 loss: 1.4152 - mae: 0.8573 - val_loss: 17.9944 - val_mae: 2.7769\n",
            "Epoch 285/500 loss: 1.4806 - mae: 0.9029 - val_loss: 17.5851 - val_mae: 2.7550\n",
            "Epoch 286/500 loss: 1.4286 - mae: 0.8111 - val_loss: 17.9942 - val_mae: 2.7712\n",
            "Epoch 287/500 loss: 1.4836 - mae: 0.8524 - val_loss: 18.4154 - val_mae: 2.8171\n",
            "Epoch 288/500 loss: 1.4561 - mae: 0.8814 - val_loss: 18.3434 - val_mae: 2.7897\n",
            "Epoch 289/500 loss: 1.3579 - mae: 0.8300 - val_loss: 18.2263 - val_mae: 2.7707\n",
            "Epoch 290/500 loss: 1.3366 - mae: 0.8239 - val_loss: 18.9281 - val_mae: 2.8244\n",
            "Epoch 291/500 loss: 1.3260 - mae: 0.8334 - val_loss: 18.1803 - val_mae: 2.7770\n",
            "Epoch 292/500 loss: 1.3262 - mae: 0.8031 - val_loss: 18.5493 - val_mae: 2.8077\n",
            "Epoch 293/500 loss: 1.3297 - mae: 0.8271 - val_loss: 17.5283 - val_mae: 2.7918\n",
            "Epoch 294/500 loss: 1.4373 - mae: 0.8546 - val_loss: 18.3025 - val_mae: 2.8185\n",
            "Epoch 295/500 loss: 1.4118 - mae: 0.8553 - val_loss: 18.0997 - val_mae: 2.8216\n",
            "Epoch 296/500 loss: 1.5171 - mae: 0.8822 - val_loss: 18.6709 - val_mae: 2.8305\n",
            "Epoch 297/500 loss: 1.3424 - mae: 0.8535 - val_loss: 19.0617 - val_mae: 2.8501\n",
            "Epoch 298/500 loss: 1.3194 - mae: 0.8414 - val_loss: 18.3890 - val_mae: 2.8467\n",
            "Epoch 299/500 loss: 1.3094 - mae: 0.8036 - val_loss: 18.4730 - val_mae: 2.7890\n",
            "Epoch 300/500 loss: 1.3646 - mae: 0.8371 - val_loss: 19.1304 - val_mae: 2.8596\n",
            "Epoch 301/500 loss: 1.3884 - mae: 0.8455 - val_loss: 17.9695 - val_mae: 2.7806\n",
            "Epoch 302/500 loss: 1.3440 - mae: 0.8261 - val_loss: 18.1165 - val_mae: 2.7834\n",
            "Epoch 303/500 loss: 1.2806 - mae: 0.8009 - val_loss: 17.8923 - val_mae: 2.7887\n",
            "Epoch 304/500 loss: 1.2883 - mae: 0.8098 - val_loss: 18.6964 - val_mae: 2.8051\n",
            "Epoch 305/500 loss: 1.2673 - mae: 0.7947 - val_loss: 18.0463 - val_mae: 2.7842\n",
            "Epoch 306/500 loss: 1.2664 - mae: 0.8085 - val_loss: 18.6207 - val_mae: 2.8047\n",
            "Epoch 307/500 loss: 1.3383 - mae: 0.8185 - val_loss: 18.0653 - val_mae: 2.7752\n",
            "Epoch 308/500 loss: 1.2733 - mae: 0.8054 - val_loss: 18.5767 - val_mae: 2.8074\n",
            "Epoch 309/500 loss: 1.4934 - mae: 0.8038 - val_loss: 18.2248 - val_mae: 2.8137\n",
            "Epoch 310/500 loss: 1.4013 - mae: 0.8236 - val_loss: 18.2377 - val_mae: 2.8030\n",
            "Epoch 311/500 loss: 1.3357 - mae: 0.8406 - val_loss: 18.3716 - val_mae: 2.7984\n",
            "Epoch 312/500 loss: 1.5695 - mae: 0.8340 - val_loss: 18.0165 - val_mae: 2.8039\n",
            "Epoch 313/500 loss: 1.3584 - mae: 0.8629 - val_loss: 18.4653 - val_mae: 2.8071\n",
            "Epoch 314/500 loss: 1.2677 - mae: 0.7874 - val_loss: 18.6020 - val_mae: 2.8256\n",
            "Epoch 315/500 loss: 1.2748 - mae: 0.8237 - val_loss: 18.9791 - val_mae: 2.8145\n",
            "Epoch 316/500 loss: 1.3013 - mae: 0.7951 - val_loss: 18.4451 - val_mae: 2.8261\n",
            "Epoch 317/500 loss: 1.4755 - mae: 0.7997 - val_loss: 18.3786 - val_mae: 2.8263\n",
            "Epoch 318/500 loss: 1.3421 - mae: 0.7998 - val_loss: 19.0856 - val_mae: 2.8721\n",
            "Epoch 319/500 loss: 1.3453 - mae: 0.8365 - val_loss: 18.6615 - val_mae: 2.8140\n",
            "Epoch 320/500 loss: 1.3573 - mae: 0.8386 - val_loss: 19.5590 - val_mae: 2.8699\n",
            "Epoch 321/500 loss: 1.3159 - mae: 0.8374 - val_loss: 17.9236 - val_mae: 2.8168\n",
            "Epoch 322/500 loss: 1.4378 - mae: 0.8828 - val_loss: 19.8317 - val_mae: 2.8433\n",
            "Epoch 323/500 loss: 1.4267 - mae: 0.8966 - val_loss: 18.2516 - val_mae: 2.7999\n",
            "Epoch 324/500 loss: 1.2505 - mae: 0.8085 - val_loss: 17.8991 - val_mae: 2.7697\n",
            "Epoch 325/500 loss: 1.1880 - mae: 0.7901 - val_loss: 18.8503 - val_mae: 2.8176\n",
            "Epoch 326/500 loss: 1.2907 - mae: 0.7799 - val_loss: 17.8860 - val_mae: 2.7675\n",
            "Epoch 327/500 loss: 1.3179 - mae: 0.8470 - val_loss: 18.7262 - val_mae: 2.8110\n",
            "Epoch 328/500 loss: 1.5337 - mae: 0.8304 - val_loss: 18.2610 - val_mae: 2.7865\n",
            "Epoch 329/500 loss: 1.7994 - mae: 0.9745 - val_loss: 17.8746 - val_mae: 2.8024\n",
            "Epoch 330/500 loss: 1.4384 - mae: 0.8637 - val_loss: 19.1133 - val_mae: 2.8118\n",
            "Epoch 331/500 loss: 1.1739 - mae: 0.7712 - val_loss: 18.2967 - val_mae: 2.8079\n",
            "Epoch 332/500 loss: 1.1980 - mae: 0.7803 - val_loss: 18.3389 - val_mae: 2.8074\n",
            "Epoch 333/500 loss: 1.1741 - mae: 0.7666 - val_loss: 18.0803 - val_mae: 2.8308\n",
            "Epoch 334/500 loss: 1.2967 - mae: 0.7910 - val_loss: 19.0714 - val_mae: 2.8204\n",
            "Epoch 335/500 loss: 1.1123 - mae: 0.7656 - val_loss: 19.2118 - val_mae: 2.8309\n",
            "Epoch 336/500 loss: 1.1701 - mae: 0.7804 - val_loss: 18.3115 - val_mae: 2.8220\n",
            "Epoch 337/500 loss: 1.0624 - mae: 0.7490 - val_loss: 18.3723 - val_mae: 2.8750\n",
            "Epoch 338/500 loss: 1.2587 - mae: 0.8201 - val_loss: 17.9961 - val_mae: 2.7858\n",
            "Epoch 339/500 loss: 1.1249 - mae: 0.7714 - val_loss: 18.2500 - val_mae: 2.7867\n",
            "Epoch 340/500 loss: 1.0923 - mae: 0.7356 - val_loss: 18.6022 - val_mae: 2.8104\n",
            "Epoch 341/500 loss: 1.0573 - mae: 0.7349 - val_loss: 18.2048 - val_mae: 2.8325\n",
            "Epoch 342/500 loss: 1.1295 - mae: 0.7415 - val_loss: 18.9084 - val_mae: 2.8361\n",
            "Epoch 343/500 loss: 1.3529 - mae: 0.8388 - val_loss: 17.4644 - val_mae: 2.7546\n",
            "Epoch 344/500 loss: 1.1827 - mae: 0.7891 - val_loss: 18.7257 - val_mae: 2.7992\n",
            "Epoch 345/500 loss: 1.2402 - mae: 0.7561 - val_loss: 18.1906 - val_mae: 2.7974\n",
            "Epoch 346/500 loss: 1.3904 - mae: 0.8374 - val_loss: 18.6994 - val_mae: 2.8353\n",
            "Epoch 347/500 loss: 1.2363 - mae: 0.8148 - val_loss: 19.0777 - val_mae: 2.8182\n",
            "Epoch 348/500 loss: 1.0801 - mae: 0.7274 - val_loss: 18.3695 - val_mae: 2.8127\n",
            "Epoch 349/500 loss: 1.0957 - mae: 0.7376 - val_loss: 18.0328 - val_mae: 2.7883\n",
            "Epoch 350/500 loss: 1.1051 - mae: 0.7529 - val_loss: 18.8061 - val_mae: 2.8162\n",
            "Epoch 351/500 loss: 1.1391 - mae: 0.7831 - val_loss: 18.3795 - val_mae: 2.8579\n",
            "Epoch 352/500 loss: 1.2226 - mae: 0.7930 - val_loss: 18.1041 - val_mae: 2.7803\n",
            "Epoch 353/500 loss: 1.1085 - mae: 0.7423 - val_loss: 17.8242 - val_mae: 2.7829\n",
            "Epoch 354/500 loss: 1.1018 - mae: 0.7324 - val_loss: 18.0992 - val_mae: 2.7777\n",
            "Epoch 355/500 loss: 1.1180 - mae: 0.7757 - val_loss: 18.5855 - val_mae: 2.8485\n",
            "Epoch 356/500 loss: 1.0928 - mae: 0.7420 - val_loss: 18.3645 - val_mae: 2.8179\n",
            "Epoch 357/500 loss: 1.1031 - mae: 0.7705 - val_loss: 17.7589 - val_mae: 2.7839\n",
            "Epoch 358/500 loss: 1.1958 - mae: 0.7867 - val_loss: 18.6558 - val_mae: 2.8435\n",
            "Epoch 359/500 loss: 1.1424 - mae: 0.7720 - val_loss: 19.1028 - val_mae: 2.8246\n",
            "Epoch 360/500 loss: 1.0656 - mae: 0.7515 - val_loss: 18.7101 - val_mae: 2.8199\n",
            "Epoch 361/500 loss: 1.0013 - mae: 0.7206 - val_loss: 18.2128 - val_mae: 2.7844\n",
            "Epoch 362/500 loss: 1.0155 - mae: 0.7197 - val_loss: 19.2217 - val_mae: 2.8528\n",
            "Epoch 363/500 loss: 1.0783 - mae: 0.7682 - val_loss: 17.9546 - val_mae: 2.7992\n",
            "Epoch 364/500 loss: 1.1072 - mae: 0.7355 - val_loss: 18.6279 - val_mae: 2.8099\n",
            "Epoch 365/500 loss: 1.0903 - mae: 0.6968 - val_loss: 18.1806 - val_mae: 2.8189\n",
            "Epoch 366/500 loss: 1.1683 - mae: 0.7997 - val_loss: 18.8627 - val_mae: 2.8174\n",
            "Epoch 367/500 loss: 1.0478 - mae: 0.7502 - val_loss: 18.9998 - val_mae: 2.8099\n",
            "Epoch 368/500 loss: 1.0123 - mae: 0.7156 - val_loss: 17.9762 - val_mae: 2.8034\n",
            "Epoch 369/500 loss: 1.2155 - mae: 0.7902 - val_loss: 19.1029 - val_mae: 2.8259\n",
            "Epoch 370/500 loss: 1.0590 - mae: 0.7257 - val_loss: 18.6781 - val_mae: 2.8109\n",
            "Epoch 371/500 loss: 1.0942 - mae: 0.7352 - val_loss: 18.0036 - val_mae: 2.8057\n",
            "Epoch 372/500 loss: 1.2088 - mae: 0.8120 - val_loss: 18.4388 - val_mae: 2.8040\n",
            "Epoch 373/500 loss: 1.2053 - mae: 0.7993 - val_loss: 18.5384 - val_mae: 2.8228\n",
            "Epoch 374/500 loss: 1.3139 - mae: 0.8480 - val_loss: 17.9580 - val_mae: 2.7663\n",
            "Epoch 375/500 loss: 1.0935 - mae: 0.7776 - val_loss: 18.4788 - val_mae: 2.7955\n",
            "Epoch 376/500 loss: 1.2271 - mae: 0.7873 - val_loss: 18.4772 - val_mae: 2.7825\n",
            "Epoch 377/500 loss: 1.1349 - mae: 0.7270 - val_loss: 18.2629 - val_mae: 2.7735\n",
            "Epoch 378/500 loss: 1.0503 - mae: 0.7297 - val_loss: 18.3890 - val_mae: 2.7992\n",
            "Epoch 379/500 loss: 0.9466 - mae: 0.6975 - val_loss: 18.2708 - val_mae: 2.7896\n",
            "Epoch 380/500 loss: 0.9432 - mae: 0.6779 - val_loss: 18.8381 - val_mae: 2.8306\n",
            "Epoch 381/500 loss: 1.0479 - mae: 0.7457 - val_loss: 18.1369 - val_mae: 2.8296\n",
            "Epoch 382/500 loss: 1.1424 - mae: 0.7766 - val_loss: 18.7054 - val_mae: 2.7912\n",
            "Epoch 383/500 loss: 0.9690 - mae: 0.7016 - val_loss: 18.8863 - val_mae: 2.7743\n",
            "Epoch 384/500 loss: 0.9083 - mae: 0.6763 - val_loss: 18.2059 - val_mae: 2.7725\n",
            "Epoch 385/500 loss: 1.0196 - mae: 0.7507 - val_loss: 18.4405 - val_mae: 2.7920\n",
            "Epoch 386/500 loss: 0.9883 - mae: 0.6878 - val_loss: 18.0404 - val_mae: 2.7978\n",
            "Epoch 387/500 loss: 0.9048 - mae: 0.6859 - val_loss: 18.0391 - val_mae: 2.7622\n",
            "Epoch 388/500 loss: 0.9192 - mae: 0.6768 - val_loss: 18.3161 - val_mae: 2.7668\n",
            "Epoch 389/500 loss: 0.9073 - mae: 0.6883 - val_loss: 18.5268 - val_mae: 2.7797\n",
            "Epoch 390/500 loss: 0.9467 - mae: 0.6861 - val_loss: 18.9662 - val_mae: 2.8101\n",
            "Epoch 391/500 loss: 0.8743 - mae: 0.6550 - val_loss: 18.6163 - val_mae: 2.8250\n",
            "Epoch 392/500 loss: 0.9731 - mae: 0.6940 - val_loss: 17.5945 - val_mae: 2.7499\n",
            "Epoch 393/500 loss: 0.9264 - mae: 0.6993 - val_loss: 18.6373 - val_mae: 2.7983\n",
            "Epoch 394/500 loss: 0.9014 - mae: 0.6612 - val_loss: 18.3318 - val_mae: 2.7862\n",
            "Epoch 395/500 loss: 1.0245 - mae: 0.7170 - val_loss: 17.8357 - val_mae: 2.7937\n",
            "Epoch 396/500 loss: 1.0300 - mae: 0.7493 - val_loss: 19.1493 - val_mae: 2.7741\n",
            "Epoch 397/500 loss: 0.9052 - mae: 0.7048 - val_loss: 18.2959 - val_mae: 2.7823\n",
            "Epoch 398/500 loss: 0.9093 - mae: 0.6999 - val_loss: 18.4618 - val_mae: 2.7515\n",
            "Epoch 399/500 loss: 0.9251 - mae: 0.6970 - val_loss: 18.3060 - val_mae: 2.7872\n",
            "Epoch 400/500 loss: 0.9193 - mae: 0.6920 - val_loss: 18.4424 - val_mae: 2.7694\n",
            "Epoch 401/500 loss: 1.1584 - mae: 0.7195 - val_loss: 18.0491 - val_mae: 2.7481\n",
            "Epoch 402/500 loss: 0.9681 - mae: 0.6898 - val_loss: 18.4960 - val_mae: 2.7736\n",
            "Epoch 403/500 loss: 0.9775 - mae: 0.7314 - val_loss: 18.1127 - val_mae: 2.7623\n",
            "Epoch 404/500 loss: 1.0071 - mae: 0.7207 - val_loss: 19.1607 - val_mae: 2.8593\n",
            "Epoch 405/500 loss: 1.2460 - mae: 0.7504 - val_loss: 19.3629 - val_mae: 2.8238\n",
            "Epoch 406/500 loss: 1.0369 - mae: 0.7515 - val_loss: 18.9293 - val_mae: 2.8471\n",
            "Epoch 407/500 loss: 0.8515 - mae: 0.6516 - val_loss: 18.8006 - val_mae: 2.7819\n",
            "Epoch 408/500 loss: 0.8785 - mae: 0.6735 - val_loss: 18.1124 - val_mae: 2.7882\n",
            "Epoch 409/500 loss: 0.8720 - mae: 0.6637 - val_loss: 18.4408 - val_mae: 2.7610\n",
            "Epoch 410/500 loss: 0.9120 - mae: 0.6784 - val_loss: 18.2008 - val_mae: 2.7907\n",
            "Epoch 411/500 loss: 0.8832 - mae: 0.6916 - val_loss: 19.2889 - val_mae: 2.8002\n",
            "Epoch 412/500 loss: 1.0012 - mae: 0.7391 - val_loss: 18.0842 - val_mae: 2.7875\n",
            "Epoch 413/500 loss: 1.0303 - mae: 0.7379 - val_loss: 18.5058 - val_mae: 2.8120\n",
            "Epoch 414/500 loss: 0.9943 - mae: 0.7014 - val_loss: 17.7964 - val_mae: 2.7485\n",
            "Epoch 415/500 loss: 1.2312 - mae: 0.8189 - val_loss: 18.6287 - val_mae: 2.7920\n",
            "Epoch 416/500 loss: 0.9186 - mae: 0.6999 - val_loss: 18.5763 - val_mae: 2.8081\n",
            "Epoch 417/500 loss: 0.8815 - mae: 0.6605 - val_loss: 18.0985 - val_mae: 2.7683\n",
            "Epoch 418/500 loss: 0.8363 - mae: 0.6560 - val_loss: 18.6857 - val_mae: 2.7962\n",
            "Epoch 419/500 loss: 0.7831 - mae: 0.6153 - val_loss: 18.4364 - val_mae: 2.7760\n",
            "Epoch 420/500 loss: 0.9147 - mae: 0.6787 - val_loss: 18.7800 - val_mae: 2.8112\n",
            "Epoch 421/500 loss: 0.8127 - mae: 0.6540 - val_loss: 18.6477 - val_mae: 2.7763\n",
            "Epoch 422/500 loss: 0.8063 - mae: 0.6486 - val_loss: 18.6943 - val_mae: 2.8032\n",
            "Epoch 423/500 loss: 0.8341 - mae: 0.6559 - val_loss: 18.5571 - val_mae: 2.7576\n",
            "Epoch 424/500 loss: 0.8596 - mae: 0.6688 - val_loss: 18.0679 - val_mae: 2.7529\n",
            "Epoch 425/500 loss: 0.8414 - mae: 0.6730 - val_loss: 18.3678 - val_mae: 2.7809\n",
            "Epoch 426/500 loss: 0.8313 - mae: 0.6709 - val_loss: 18.9270 - val_mae: 2.8285\n",
            "Epoch 427/500 loss: 0.8116 - mae: 0.6335 - val_loss: 18.8768 - val_mae: 2.7861\n",
            "Epoch 428/500 loss: 0.7703 - mae: 0.6209 - val_loss: 18.6645 - val_mae: 2.8091\n",
            "Epoch 429/500 loss: 0.9390 - mae: 0.7179 - val_loss: 19.5435 - val_mae: 2.8052\n",
            "Epoch 430/500 loss: 0.8229 - mae: 0.6732 - val_loss: 18.4086 - val_mae: 2.7569\n",
            "Epoch 431/500 loss: 0.8325 - mae: 0.6464 - val_loss: 19.3312 - val_mae: 2.8303\n",
            "Epoch 432/500 loss: 0.9021 - mae: 0.6798 - val_loss: 18.1826 - val_mae: 2.7483\n",
            "Epoch 433/500 loss: 0.8735 - mae: 0.6820 - val_loss: 18.7500 - val_mae: 2.8298\n",
            "Epoch 434/500 loss: 0.8671 - mae: 0.6709 - val_loss: 18.1858 - val_mae: 2.7450\n",
            "Epoch 435/500 loss: 0.7701 - mae: 0.6256 - val_loss: 19.6058 - val_mae: 2.8524\n",
            "Epoch 436/500 loss: 0.8221 - mae: 0.6437 - val_loss: 18.4768 - val_mae: 2.7865\n",
            "Epoch 437/500 loss: 0.7497 - mae: 0.6369 - val_loss: 20.0982 - val_mae: 2.8728\n",
            "Epoch 438/500 loss: 0.7822 - mae: 0.6339 - val_loss: 18.2133 - val_mae: 2.7804\n",
            "Epoch 439/500 loss: 0.8184 - mae: 0.6624 - val_loss: 18.7638 - val_mae: 2.8031\n",
            "Epoch 440/500 loss: 0.7239 - mae: 0.6123 - val_loss: 18.4724 - val_mae: 2.7934\n",
            "Epoch 441/500 loss: 0.8109 - mae: 0.6468 - val_loss: 18.4728 - val_mae: 2.7761\n",
            "Epoch 442/500 loss: 0.9189 - mae: 0.6760 - val_loss: 18.1343 - val_mae: 2.7884\n",
            "Epoch 443/500 loss: 1.0735 - mae: 0.7039 - val_loss: 19.5614 - val_mae: 2.8437\n",
            "Epoch 444/500 loss: 0.9416 - mae: 0.7128 - val_loss: 18.3002 - val_mae: 2.7751\n",
            "Epoch 445/500 loss: 0.8318 - mae: 0.6560 - val_loss: 18.4093 - val_mae: 2.7926\n",
            "Epoch 446/500 loss: 0.7935 - mae: 0.6340 - val_loss: 17.9802 - val_mae: 2.7487\n",
            "Epoch 447/500 loss: 0.7890 - mae: 0.6620 - val_loss: 18.9605 - val_mae: 2.7914\n",
            "Epoch 448/500 loss: 0.7928 - mae: 0.6517 - val_loss: 18.0128 - val_mae: 2.7643\n",
            "Epoch 449/500 loss: 0.8274 - mae: 0.6402 - val_loss: 19.1787 - val_mae: 2.8204\n",
            "Epoch 450/500 loss: 0.7520 - mae: 0.6285 - val_loss: 18.9689 - val_mae: 2.8071\n",
            "Epoch 451/500 loss: 0.7239 - mae: 0.6077 - val_loss: 18.8001 - val_mae: 2.8207\n",
            "Epoch 452/500 loss: 0.7204 - mae: 0.6017 - val_loss: 18.7963 - val_mae: 2.8269\n",
            "Epoch 453/500 loss: 0.7690 - mae: 0.6331 - val_loss: 17.9355 - val_mae: 2.7831\n",
            "Epoch 454/500 loss: 0.7417 - mae: 0.6197 - val_loss: 19.4274 - val_mae: 2.8224\n",
            "Epoch 455/500 loss: 0.8062 - mae: 0.6502 - val_loss: 18.6991 - val_mae: 2.8020\n",
            "Epoch 456/500 loss: 0.7573 - mae: 0.6128 - val_loss: 18.2502 - val_mae: 2.7627\n",
            "Epoch 457/500 loss: 0.8526 - mae: 0.6837 - val_loss: 19.1613 - val_mae: 2.7977\n",
            "Epoch 458/500 loss: 0.7010 - mae: 0.6066 - val_loss: 17.9209 - val_mae: 2.7715\n",
            "Epoch 459/500 loss: 0.7324 - mae: 0.6132 - val_loss: 18.5986 - val_mae: 2.7877\n",
            "Epoch 460/500 loss: 0.6697 - mae: 0.5765 - val_loss: 18.9509 - val_mae: 2.8015\n",
            "Epoch 461/500 loss: 0.8849 - mae: 0.6501 - val_loss: 18.3330 - val_mae: 2.7682\n",
            "Epoch 462/500 loss: 1.0555 - mae: 0.7303 - val_loss: 18.7423 - val_mae: 2.8263\n",
            "Epoch 463/500 loss: 1.0539 - mae: 0.7462 - val_loss: 19.8452 - val_mae: 2.8694\n",
            "Epoch 464/500 loss: 0.8782 - mae: 0.6785 - val_loss: 18.0316 - val_mae: 2.7611\n",
            "Epoch 465/500 loss: 0.8938 - mae: 0.6693 - val_loss: 18.9434 - val_mae: 2.8068\n",
            "Epoch 466/500 loss: 0.8940 - mae: 0.6832 - val_loss: 18.9862 - val_mae: 2.7996\n",
            "Epoch 467/500 loss: 0.8173 - mae: 0.6724 - val_loss: 18.5681 - val_mae: 2.7966\n",
            "Epoch 468/500 loss: 0.8254 - mae: 0.6577 - val_loss: 18.2465 - val_mae: 2.7646\n",
            "Epoch 469/500 loss: 0.7173 - mae: 0.5927 - val_loss: 18.7691 - val_mae: 2.7820\n",
            "Epoch 470/500 loss: 0.7077 - mae: 0.5761 - val_loss: 18.6078 - val_mae: 2.7928\n",
            "Epoch 471/500 loss: 0.6985 - mae: 0.6060 - val_loss: 18.3352 - val_mae: 2.7704\n",
            "Epoch 472/500 loss: 0.6867 - mae: 0.5883 - val_loss: 18.7887 - val_mae: 2.7944\n",
            "Epoch 473/500 loss: 0.7615 - mae: 0.6363 - val_loss: 18.7336 - val_mae: 2.7831\n",
            "Epoch 474/500 loss: 0.7052 - mae: 0.6140 - val_loss: 18.6565 - val_mae: 2.8101\n",
            "Epoch 475/500 loss: 0.6603 - mae: 0.5687 - val_loss: 18.9739 - val_mae: 2.8274\n",
            "Epoch 476/500 loss: 0.7557 - mae: 0.6278 - val_loss: 18.6253 - val_mae: 2.7836\n",
            "Epoch 477/500 loss: 0.7749 - mae: 0.6551 - val_loss: 19.4741 - val_mae: 2.8067\n",
            "Epoch 478/500 loss: 0.7939 - mae: 0.6492 - val_loss: 19.5584 - val_mae: 2.8248\n",
            "Epoch 479/500 loss: 0.6949 - mae: 0.5591 - val_loss: 18.7218 - val_mae: 2.7944\n",
            "Epoch 480/500 loss: 0.6508 - mae: 0.5721 - val_loss: 18.6050 - val_mae: 2.7674\n",
            "Epoch 481/500 loss: 0.6553 - mae: 0.5734 - val_loss: 18.9865 - val_mae: 2.8273\n",
            "Epoch 482/500 loss: 0.6714 - mae: 0.5684 - val_loss: 18.6584 - val_mae: 2.8188\n",
            "Epoch 483/500 loss: 0.6646 - mae: 0.5878 - val_loss: 18.7174 - val_mae: 2.8033\n",
            "Epoch 484/500 loss: 0.6235 - mae: 0.5529 - val_loss: 18.3200 - val_mae: 2.7756\n",
            "Epoch 485/500 loss: 0.6141 - mae: 0.5629 - val_loss: 18.4806 - val_mae: 2.7846\n",
            "Epoch 486/500 loss: 0.6364 - mae: 0.5717 - val_loss: 18.4093 - val_mae: 2.8317\n",
            "Epoch 487/500 loss: 0.6501 - mae: 0.5881 - val_loss: 19.0670 - val_mae: 2.7976\n",
            "Epoch 488/500 loss: 0.7028 - mae: 0.5951 - val_loss: 19.0867 - val_mae: 2.8207\n",
            "Epoch 489/500 loss: 0.6495 - mae: 0.5612 - val_loss: 18.9752 - val_mae: 2.8152\n",
            "Epoch 490/500 loss: 0.6646 - mae: 0.5673 - val_loss: 17.9719 - val_mae: 2.8000\n",
            "Epoch 491/500 loss: 0.8805 - mae: 0.6971 - val_loss: 19.3944 - val_mae: 2.8577\n",
            "Epoch 492/500 loss: 0.8244 - mae: 0.6417 - val_loss: 18.5459 - val_mae: 2.7626\n",
            "Epoch 493/500 loss: 0.8255 - mae: 0.6648 - val_loss: 17.4175 - val_mae: 2.7434\n",
            "Epoch 494/500 loss: 0.8735 - mae: 0.6977 - val_loss: 18.3584 - val_mae: 2.7765\n",
            "Epoch 495/500 loss: 0.7495 - mae: 0.6485 - val_loss: 19.2321 - val_mae: 2.8212\n",
            "Epoch 496/500 loss: 0.7137 - mae: 0.6126 - val_loss: 18.4880 - val_mae: 2.7557\n",
            "Epoch 497/500 loss: 0.7248 - mae: 0.6195 - val_loss: 18.4493 - val_mae: 2.8535\n",
            "Epoch 498/500 loss: 0.7312 - mae: 0.5885 - val_loss: 18.4110 - val_mae: 2.7713\n",
            "Epoch 499/500 loss: 0.9262 - mae: 0.7039 - val_loss: 18.5979 - val_mae: 2.7544\n",
            "Epoch 500/500 loss: 0.6836 - mae: 0.6176 - val_loss: 18.9793 - val_mae: 2.8120\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6176272389980463, 2.8120128767830983]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Create model.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(13, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 1)\n",
        ")\n",
        "\n",
        "# Initialize weights.\n",
        "for module in model.modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "        nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "# Loss function and optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# Train model.\n",
        "train_model(model, device, EPOCHS, BATCH_SIZE, trainset, testset,\n",
        "            optimizer, loss_function, 'mae')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaAa_JTDLasE"
      },
      "source": [
        "After the training is done, we use our model to predict the price for all test examples and then print out the first four predictions and the correct values so we can get an idea of how correct the model is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HLn3fS37LasF",
        "outputId": "274debc5-90a0-42b6-aefa-ef34490141de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: 22.92 , true value: 22.60\n",
            "Prediction: 29.77 , true value: 50.00\n",
            "Prediction: 23.79 , true value: 23.00\n",
            "Prediction: 11.60 , true value: 8.30\n"
          ]
        }
      ],
      "source": [
        "# Print first 4 predictions.\n",
        "inputs = torch.from_numpy(x_test)\n",
        "inputs = inputs.to(device)\n",
        "outputs = model(inputs)\n",
        "for i in range(0, 4):\n",
        "    print('Prediction: %4.2f' % outputs.data[i].item(),\n",
        "         ', true value: %4.2f' % y_test[i].item())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "c6e1_boston.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}